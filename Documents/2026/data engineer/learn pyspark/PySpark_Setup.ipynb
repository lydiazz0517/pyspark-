{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark 快速上手教程\n",
    "\n",
    "## 第一步：安装 PySpark\n",
    "\n",
    "在 Jupyter Notebook 中，我们使用虚拟环境或直接安装到用户目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 1：安装到用户目录（推荐）\n",
    "!pip install --user pyspark pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 环境设置：修复 Python 路径问题\n",
    "# ============================================================\n",
    "import sys\n",
    "\n",
    "# 添加必要的路径到 sys.path\n",
    "paths_to_add = [\n",
    "    '/Users/zhengzhang/.local/lib/python3.9/site-packages',  # PySpark 位置\n",
    "    '/Users/zhengzhang/opt/anaconda3/lib/python3.9/site-packages'  # Matplotlib, Pandas, Numpy 位置\n",
    "]\n",
    "\n",
    "for path in paths_to_add:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "        print(f\"✓ 已添加路径: {path}\")\n",
    "\n",
    "print(\"\\n验证所有依赖包...\")\n",
    "\n",
    "# 验证 PySpark\n",
    "import pyspark\n",
    "print(f\"✓ PySpark 版本: {pyspark.__version__}\")\n",
    "\n",
    "# 验证其他依赖\n",
    "import pandas as pd\n",
    "print(f\"✓ Pandas 版本: {pd.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "print(f\"✓ Numpy 版本: {np.__version__}\")\n",
    "\n",
    "import matplotlib\n",
    "print(f\"✓ Matplotlib 版本: {matplotlib.__version__}\")\n",
    "\n",
    "print(\"\\n所有依赖包已就绪！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证安装\n",
    "import pyspark\n",
    "print(f\"✓ PySpark 版本: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：创建 Spark Session\n",
    "\n",
    "这是使用 PySpark 的第一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建 Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark 学习\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark Session 创建成功！\")\n",
    "print(f\"✓ Spark 版本: {spark.version}\")\n",
    "print(f\"✓ Spark Web UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 1：星型模式（Star Schema）数据仓库\n",
    "\n",
    "这是数据仓库中最常用的模型，包含：\n",
    "- **1个事实表**（Fact Table）：存储业务度量值和外键\n",
    "- **多个维度表**（Dimension Tables）：存储描述性信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 星型模式（Star Schema）示例 - 电商销售数据仓库\n",
    "# ============================================================\n",
    "\n",
    "# 1. 事实表（Fact Table）- 销售事实表\n",
    "# 包含度量值（销售额、数量等）和维度表的外键\n",
    "sales_fact_data = [\n",
    "    (1, 101, 201, 301, 401, \"2024-01-15\", 2, 299.98, 59.99, 359.97),\n",
    "    (2, 102, 202, 302, 402, \"2024-01-15\", 1, 899.00, 0.00, 899.00),\n",
    "    (3, 103, 203, 303, 401, \"2024-01-16\", 3, 149.97, 15.00, 164.97),\n",
    "    (4, 101, 204, 304, 403, \"2024-01-16\", 1, 599.00, 59.90, 658.90),\n",
    "    (5, 104, 205, 301, 402, \"2024-01-17\", 5, 249.95, 0.00, 249.95),\n",
    "    (6, 105, 201, 305, 404, \"2024-01-17\", 2, 1799.98, 180.00, 1979.98),\n",
    "    (7, 102, 206, 302, 401, \"2024-01-18\", 1, 1299.00, 129.90, 1428.90),\n",
    "    (8, 106, 207, 303, 403, \"2024-01-18\", 4, 199.96, 0.00, 199.96),\n",
    "    (9, 103, 208, 306, 402, \"2024-01-19\", 1, 79.99, 0.00, 79.99),\n",
    "    (10, 107, 209, 301, 404, \"2024-01-19\", 2, 399.98, 40.00, 439.98)\n",
    "]\n",
    "\n",
    "sales_fact_columns = [\n",
    "    \"订单ID\", \"客户ID\", \"产品ID\", \"商店ID\", \"时间ID\", \n",
    "    \"订单日期\", \"数量\", \"销售金额\", \"折扣金额\", \"总金额\"\n",
    "]\n",
    "\n",
    "fact_sales = spark.createDataFrame(sales_fact_data, sales_fact_columns)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"事实表：销售事实表 (Fact_Sales)\")\n",
    "print(\"=\" * 60)\n",
    "fact_sales.show()\n",
    "\n",
    "# 2. 维度表 1 - 客户维度（Dim_Customer）\n",
    "customer_dim_data = [\n",
    "    (101, \"张三\", \"男\", 28, \"北京\", \"海淀区\", \"VIP\"),\n",
    "    (102, \"李四\", \"女\", 35, \"上海\", \"浦东新区\", \"普通\"),\n",
    "    (103, \"王五\", \"男\", 42, \"广州\", \"天河区\", \"VIP\"),\n",
    "    (104, \"赵六\", \"女\", 25, \"深圳\", \"南山区\", \"普通\"),\n",
    "    (105, \"钱七\", \"男\", 31, \"杭州\", \"西湖区\", \"黄金\"),\n",
    "    (106, \"孙八\", \"女\", 29, \"成都\", \"武侯区\", \"VIP\"),\n",
    "    (107, \"周九\", \"男\", 38, \"北京\", \"朝阳区\", \"黄金\")\n",
    "]\n",
    "\n",
    "customer_dim_columns = [\"客户ID\", \"客户姓名\", \"性别\", \"年龄\", \"城市\", \"区域\", \"会员等级\"]\n",
    "dim_customer = spark.createDataFrame(customer_dim_data, customer_dim_columns)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"维度表 1：客户维度 (Dim_Customer)\")\n",
    "print(\"=\" * 60)\n",
    "dim_customer.show()\n",
    "\n",
    "# 3. 维度表 2 - 产品维度（Dim_Product）\n",
    "product_dim_data = [\n",
    "    (201, \"iPhone 15\", \"电子产品\", \"手机\", \"Apple\", 6999.00),\n",
    "    (202, \"MacBook Pro\", \"电子产品\", \"笔记本\", \"Apple\", 12999.00),\n",
    "    (203, \"小米手环\", \"电子产品\", \"可穿戴\", \"小米\", 199.00),\n",
    "    (204, \"戴尔显示器\", \"电子产品\", \"显示器\", \"Dell\", 2999.00),\n",
    "    (205, \"罗技鼠标\", \"电子产品\", \"外设\", \"罗技\", 149.00),\n",
    "    (206, \"索尼耳机\", \"电子产品\", \"音频\", \"Sony\", 1899.00),\n",
    "    (207, \"键盘机械\", \"电子产品\", \"外设\", \"Cherry\", 699.00),\n",
    "    (208, \"充电宝\", \"电子产品\", \"配件\", \"Anker\", 199.00),\n",
    "    (209, \"iPad Air\", \"电子产品\", \"平板\", \"Apple\", 4999.00)\n",
    "]\n",
    "\n",
    "product_dim_columns = [\"产品ID\", \"产品名称\", \"产品类别\", \"产品子类别\", \"品牌\", \"单价\"]\n",
    "dim_product = spark.createDataFrame(product_dim_data, product_dim_columns)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"维度表 2：产品维度 (Dim_Product)\")\n",
    "print(\"=\" * 60)\n",
    "dim_product.show()\n",
    "\n",
    "# 4. 维度表 3 - 商店维度（Dim_Store）\n",
    "store_dim_data = [\n",
    "    (301, \"北京旗舰店\", \"北京\", \"海淀区\", \"中关村大街1号\", \"大型\", \"张经理\"),\n",
    "    (302, \"上海体验店\", \"上海\", \"浦东新区\", \"陆家嘴环路999号\", \"中型\", \"李经理\"),\n",
    "    (303, \"广州专卖店\", \"广州\", \"天河区\", \"天河路123号\", \"小型\", \"王经理\"),\n",
    "    (304, \"深圳科技店\", \"深圳\", \"南山区\", \"科技园路88号\", \"大型\", \"赵经理\"),\n",
    "    (305, \"杭州电商仓\", \"杭州\", \"余杭区\", \"文一西路999号\", \"仓库\", \"钱经理\"),\n",
    "    (306, \"成都门店\", \"成都\", \"武侯区\", \"科华北路66号\", \"中型\", \"孙经理\")\n",
    "]\n",
    "\n",
    "store_dim_columns = [\"商店ID\", \"商店名称\", \"城市\", \"区域\", \"地址\", \"商店类型\", \"店长\"]\n",
    "dim_store = spark.createDataFrame(store_dim_data, store_dim_columns)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"维度表 3：商店维度 (Dim_Store)\")\n",
    "print(\"=\" * 60)\n",
    "dim_store.show()\n",
    "\n",
    "# 5. 维度表 4 - 时间维度（Dim_Time）\n",
    "time_dim_data = [\n",
    "    (401, \"2024-01-15\", 2024, 1, \"一月\", 3, 1, \"星期一\", \"工作日\", \"Q1\"),\n",
    "    (402, \"2024-01-16\", 2024, 1, \"一月\", 3, 2, \"星期二\", \"工作日\", \"Q1\"),\n",
    "    (403, \"2024-01-17\", 2024, 1, \"一月\", 3, 3, \"星期三\", \"工作日\", \"Q1\"),\n",
    "    (404, \"2024-01-18\", 2024, 1, \"一月\", 3, 4, \"星期四\", \"工作日\", \"Q1\")\n",
    "]\n",
    "\n",
    "time_dim_columns = [\n",
    "    \"时间ID\", \"日期\", \"年\", \"月\", \"月份名称\", \n",
    "    \"周数\", \"星期几\", \"星期名称\", \"日类型\", \"季度\"\n",
    "]\n",
    "dim_time = spark.createDataFrame(time_dim_data, time_dim_columns)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"维度表 4：时间维度 (Dim_Time)\")\n",
    "print(\"=\" * 60)\n",
    "dim_time.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"星型模式结构说明\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "中心：事实表 (Fact_Sales) - 包含业务度量值\n",
    "  ├─ 客户ID → Dim_Customer (客户维度)\n",
    "  ├─ 产品ID → Dim_Product (产品维度)\n",
    "  ├─ 商店ID → Dim_Store (商店维度)\n",
    "  └─ 时间ID → Dim_Time (时间维度)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 2：星型模式查询分析\n",
    "\n",
    "下面演示如何对星型模式进行多维分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions\n",
    "from pyspark.sql.functions import count, sum, avg, desc\n",
    "\n",
    "# 查询 1：按城市统计销售额（事实表 JOIN 商店维度）\n",
    "print(\"=\" * 60)\n",
    "print(\"查询 1：各城市销售统计\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "city_sales = fact_sales.join(dim_store, \"商店ID\") \\\n",
    "    .groupBy(\"城市\") \\\n",
    "    .agg(\n",
    "        count(\"订单ID\").alias(\"订单数量\"),\n",
    "        sum(\"总金额\").alias(\"总销售额\"),\n",
    "        avg(\"总金额\").alias(\"平均订单金额\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"总销售额\"))\n",
    "\n",
    "city_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询 2：按产品品牌统计（事实表 JOIN 产品维度）\n",
    "from pyspark.sql.functions import col, round as spark_round\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"查询 2：各品牌销售排行\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "brand_sales = fact_sales.join(dim_product, \"产品ID\") \\\n",
    "    .groupBy(\"品牌\") \\\n",
    "    .agg(\n",
    "        count(\"订单ID\").alias(\"销售次数\"),\n",
    "        sum(\"数量\").alias(\"销售数量\"),\n",
    "        spark_round(sum(\"总金额\"), 2).alias(\"总销售额\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"总销售额\"))\n",
    "\n",
    "brand_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询 3：VIP 客户消费分析（事实表 JOIN 客户维度）\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"查询 3：按会员等级统计消费\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "customer_level_sales = fact_sales.join(dim_customer, \"客户ID\") \\\n",
    "    .groupBy(\"会员等级\") \\\n",
    "    .agg(\n",
    "        count(\"订单ID\").alias(\"订单数\"),\n",
    "        spark_round(sum(\"总金额\"), 2).alias(\"总消费\"),\n",
    "        spark_round(avg(\"总金额\"), 2).alias(\"平均消费\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"总消费\"))\n",
    "\n",
    "customer_level_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询 4：多维度联合查询 - 完整的星型模式 JOIN\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"查询 4：综合销售明细报表（JOIN 所有维度表）\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 将事实表与所有维度表 JOIN\n",
    "full_report = fact_sales \\\n",
    "    .join(dim_customer, \"客户ID\") \\\n",
    "    .join(dim_product, \"产品ID\") \\\n",
    "    .join(dim_store, \"商店ID\") \\\n",
    "    .join(dim_time, \"时间ID\") \\\n",
    "    .select(\n",
    "        \"订单ID\",\n",
    "        \"客户姓名\",\n",
    "        \"会员等级\",\n",
    "        \"产品名称\",\n",
    "        \"品牌\",\n",
    "        col(\"数量\"),\n",
    "        col(\"总金额\"),\n",
    "        dim_store[\"城市\"].alias(\"购买城市\"),\n",
    "        \"商店名称\",\n",
    "        \"星期名称\",\n",
    "        \"日类型\"\n",
    "    )\n",
    "\n",
    "full_report.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询 5：使用 SQL 进行星型模式查询\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"查询 5：SQL 方式查询星型模式\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 注册所有表为临时视图\n",
    "fact_sales.createOrReplaceTempView(\"fact_sales\")\n",
    "dim_customer.createOrReplaceTempView(\"dim_customer\")\n",
    "dim_product.createOrReplaceTempView(\"dim_product\")\n",
    "dim_store.createOrReplaceTempView(\"dim_store\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time\")\n",
    "\n",
    "# 复杂 SQL 查询：按城市、产品类别统计\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.城市,\n",
    "        p.产品类别,\n",
    "        COUNT(f.订单ID) as 订单数,\n",
    "        SUM(f.数量) as 销售数量,\n",
    "        ROUND(SUM(f.总金额), 2) as 总销售额,\n",
    "        ROUND(AVG(f.总金额), 2) as 平均订单金额\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_store s ON f.商店ID = s.商店ID\n",
    "    JOIN dim_product p ON f.产品ID = p.产品ID\n",
    "    GROUP BY s.城市, p.产品类别\n",
    "    ORDER BY 总销售额 DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 3：星型模式的优势\n",
    "\n",
    "通过上面的示例，你可以看到星型模式的优势：\n",
    "\n",
    "1. **查询性能好**：简单的 JOIN 关系，查询速度快\n",
    "2. **易于理解**：结构清晰，业务人员容易理解\n",
    "3. **灵活分析**：可以从任意维度进行分析\n",
    "4. **数据冗余少**：维度表数据不重复\n",
    "\n",
    "下面继续其他 PySpark 示例..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg, sum, max, min\n",
    "\n",
    "# 基本统计\n",
    "print(f\"总员工数: {df.count()}\")\n",
    "print(\"\\n描述性统计：\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按部门分组统计\n",
    "print(\"按部门统计：\")\n",
    "dept_stats = df.groupBy(\"部门\").agg(\n",
    "    count(\"*\").alias(\"员工数\"),\n",
    "    avg(\"薪资\").alias(\"平均薪资\"),\n",
    "    max(\"薪资\").alias(\"最高薪资\"),\n",
    "    min(\"薪资\").alias(\"最低薪资\"),\n",
    "    sum(\"薪资\").alias(\"总薪资\")\n",
    ")\n",
    "\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 4：使用 Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注册为临时表\n",
    "df.createOrReplaceTempView(\"员工表\")\n",
    "\n",
    "# 使用 SQL 查询\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT 部门,\n",
    "           COUNT(*) as 员工数,\n",
    "           ROUND(AVG(薪资), 2) as 平均薪资,\n",
    "           MAX(薪资) as 最高薪资\n",
    "    FROM 员工表\n",
    "    WHERE 年龄 > 25\n",
    "    GROUP BY 部门\n",
    "    ORDER BY 平均薪资 DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL 查询结果：\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 5：JOIN 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建部门信息表\n",
    "dept_data = [\n",
    "    (\"工程部\", \"A栋\", \"张经理\"),\n",
    "    (\"销售部\", \"B栋\", \"李经理\"),\n",
    "    (\"市场部\", \"C栋\", \"王经理\"),\n",
    "    (\"人力资源部\", \"D栋\", \"赵经理\")\n",
    "]\n",
    "\n",
    "dept_df = spark.createDataFrame(dept_data, [\"部门\", \"办公地点\", \"经理\"])\n",
    "\n",
    "print(\"部门信息：\")\n",
    "dept_df.show()\n",
    "\n",
    "# JOIN 操作\n",
    "print(\"\\n员工和部门信息 JOIN 结果：\")\n",
    "joined_df = df.join(dept_df, \"部门\", \"left\")\n",
    "joined_df.select(\"姓名\", \"部门\", \"薪资\", \"办公地点\", \"经理\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 6：自定义函数 (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 定义 Python 函数\n",
    "def categorize_age(age):\n",
    "    if age < 25:\n",
    "        return \"新人\"\n",
    "    elif age < 30:\n",
    "        return \"骨干\"\n",
    "    else:\n",
    "        return \"资深\"\n",
    "\n",
    "# 注册为 UDF\n",
    "categorize_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# 使用 UDF\n",
    "print(\"添加员工级别分类：\")\n",
    "df.withColumn(\"级别\", categorize_udf(col(\"年龄\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 7：窗口函数（部门内排名）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, desc\n",
    "\n",
    "# 定义窗口：按部门分区，按薪资降序\n",
    "window_spec = Window.partitionBy(\"部门\").orderBy(desc(\"薪资\"))\n",
    "\n",
    "# 添加排名\n",
    "print(\"部门内薪资排名：\")\n",
    "df.withColumn(\"部门内排名\", rank().over(window_spec)) \\\n",
    "  .orderBy(\"部门\", \"部门内排名\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 8：经典 WordCount（词频统计）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# 创建文本数据\n",
    "text_data = [\n",
    "    (\"Apache Spark is a unified analytics engine for big data processing\",),\n",
    "    (\"PySpark is the Python API for Apache Spark\",),\n",
    "    (\"Spark provides high level APIs in Python Java Scala and R\",),\n",
    "    (\"Spark runs on Hadoop YARN Kubernetes and standalone mode\",)\n",
    "]\n",
    "\n",
    "text_df = spark.createDataFrame(text_data, [\"text\"])\n",
    "\n",
    "print(\"原始文本：\")\n",
    "text_df.show(truncate=False)\n",
    "\n",
    "# 分词和统计\n",
    "words = text_df.select(explode(split(col(\"text\"), \" \")).alias(\"word\"))\n",
    "word_count = words.groupBy(\"word\").count().orderBy(desc(\"count\"))\n",
    "\n",
    "print(\"\\n词频统计结果（Top 10）：\")\n",
    "word_count.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 9：读写文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为 CSV\n",
    "output_path = \"/tmp/pyspark_demo\"\n",
    "\n",
    "print(\"保存为 CSV...\")\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}/employees.csv\")\n",
    "print(\"✓ CSV 保存成功\")\n",
    "\n",
    "# 保存为 Parquet（推荐格式）\n",
    "print(\"\\n保存为 Parquet...\")\n",
    "df.write.mode(\"overwrite\").parquet(f\"{output_path}/employees.parquet\")\n",
    "print(\"✓ Parquet 保存成功\")\n",
    "\n",
    "# 读取 Parquet\n",
    "print(\"\\n从 Parquet 读取：\")\n",
    "read_df = spark.read.parquet(f\"{output_path}/employees.parquet\")\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 10：数据可视化（配合 Pandas）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 将 Spark DataFrame 转换为 Pandas DataFrame\n",
    "dept_stats_pd = dept_stats.toPandas()\n",
    "\n",
    "# 绘制柱状图\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 员工数\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(dept_stats_pd['部门'], dept_stats_pd['员工数'])\n",
    "plt.title('各部门员工数')\n",
    "plt.xlabel('部门')\n",
    "plt.ylabel('员工数')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 平均薪资\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(dept_stats_pd['部门'], dept_stats_pd['平均薪资'])\n",
    "plt.title('各部门平均薪资')\n",
    "plt.xlabel('部门')\n",
    "plt.ylabel('平均薪资 (元)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n统计数据：\")\n",
    "print(dept_stats_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 将 Spark DataFrame 转换为 Pandas DataFrame\n",
    "dept_stats_pd = dept_stats.toPandas()\n",
    "\n",
    "# 绘制柱状图\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 员工数\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(dept_stats_pd['部门'], dept_stats_pd['员工数'])\n",
    "plt.title('各部门员工数')\n",
    "plt.xlabel('部门')\n",
    "plt.ylabel('员工数')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 平均薪资\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(dept_stats_pd['部门'], dept_stats_pd['平均薪资'])\n",
    "plt.title('各部门平均薪资')\n",
    "plt.xlabel('部门')\n",
    "plt.ylabel('平均薪资 (元)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n统计数据：\")\n",
    "print(dept_stats_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习题\n",
    "\n",
    "### 练习 1：数据筛选\n",
    "找出薪资在 65000 到 75000 之间的员工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你的代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 2：复杂查询\n",
    "计算每个部门最年轻和最年长员工的年龄差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你的代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 3：数据转换\n",
    "创建一个新列，将薪资转换为薪资等级：\n",
    "- < 65000: \"初级\"\n",
    "- 65000-75000: \"中级\"\n",
    "- > 75000: \"高级\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你的代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最后：关闭 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完成所有操作后关闭 Spark Session\n",
    "# spark.stop()\n",
    "# print(\"✓ Spark Session 已关闭\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 Spark 配置信息\n",
    "print(\"=\" * 60)\n",
    "print(\"Spark 运行环境信息\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Master 模式\n",
    "print(f\"运行模式: {spark.sparkContext.master}\")\n",
    "\n",
    "# 2. 应用名称\n",
    "print(f\"应用名称: {spark.sparkContext.appName}\")\n",
    "\n",
    "# 3. 默认并行度（决定分区数）\n",
    "print(f\"默认并行度: {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# 4. 查看你的 CPU 核心数\n",
    "import os\n",
    "cpu_count = os.cpu_count()\n",
    "print(f\"CPU 核心数: {cpu_count}\")\n",
    "\n",
    "# 5. 内存配置\n",
    "print(f\"Driver 内存: {spark.sparkContext._conf.get('spark.driver.memory')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"分区和 Stage 示例\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 6. 查看 DataFrame 的分区数\n",
    "print(f\"fact_sales 分区数: {fact_sales.rdd.getNumPartitions()}\")\n",
    "print(f\"dim_store 分区数: {dim_store.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 7. 解释查询执行计划（可以看到 Shuffle）\n",
    "print(\"\\n查询执行计划（Physical Plan）：\")\n",
    "city_sales.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 执行详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下一步学习\n",
    "\n",
    "1. **进阶主题**\n",
    "   - Spark Streaming（实时数据处理）\n",
    "   - Spark MLlib（机器学习）\n",
    "   - 性能优化技巧\n",
    "\n",
    "2. **实战项目**\n",
    "   - 日志分析系统\n",
    "   - 推荐系统\n",
    "   - 实时监控仪表板\n",
    "\n",
    "3. **学习资源**\n",
    "   - [官方文档](https://spark.apache.org/docs/latest/)\n",
    "   - [PySpark API](https://spark.apache.org/docs/latest/api/python/)\n",
    "   - [Databricks 免费平台](https://community.cloud.databricks.com/)\n",
    "\n",
    "加油学习！有问题随时查看文档或提问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 查看工作目录\n",
    "pwd\n",
    "# 输出: /Users/zhengzhang\n",
    "\n",
    "# 2. 清理 Notebook 输出\n",
    "jupyter nbconvert --clear-output --inplace PySpark_Setup.ipynb\n",
    "\n",
    "# 3. 查看修改状态\n",
    "git status\n",
    "# 输出: modified: PySpark_Setup.ipynb\n",
    "\n",
    "# 4. 查看具体改动（可选）\n",
    "git diff PySpark_Setup.ipynb\n",
    "\n",
    "# 5. 添加到暂存区\n",
    "git add PySpark_Setup.ipynb\n",
    "\n",
    "# 6. 提交\n",
    "git commit -m \"Add: 新增按时间维度的销售分析查询\"\n",
    "\n",
    "# 7. 推送到 GitHub\n",
    "git push\n",
    "# Username: lydiazz0517\n",
    "# Password: [粘贴 token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zhengzhang'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
