# Pandas vs Polars vs PySpark å®Œæ•´å¯¹æ¯”

## ä¸‰å¤§æ•°æ®å¤„ç†å·¥å…·æ¦‚è§ˆ

| ç‰¹æ€§ | Pandas | Polars | PySpark |
|------|--------|--------|---------|
| **å‘å¸ƒæ—¶é—´** | 2008 | 2020 | 2014 |
| **åº•å±‚è¯­è¨€** | C/Python | Rust | Scala/Java |
| **æ‰§è¡Œæ¨¡å¼** | ç«‹å³æ‰§è¡Œ | å»¶è¿Ÿæ‰§è¡Œ | å»¶è¿Ÿæ‰§è¡Œ |
| **å¤„ç†é€Ÿåº¦** | åŸºå‡† (1x) | ğŸš€ **5-10x** | åˆ†å¸ƒå¼ |
| **å†…å­˜æ•ˆç‡** | è¾ƒä½ | ğŸ”¥ **æé«˜** | åˆ†å¸ƒå¼ |
| **æ•°æ®è§„æ¨¡** | < 10GB | < 100GB | TB+ çº§åˆ« |
| **å¹¶è¡Œå¤„ç†** | å•çº¿ç¨‹ä¸ºä¸» | å¤šçº¿ç¨‹ | åˆ†å¸ƒå¼é›†ç¾¤ |
| **å­¦ä¹ æ›²çº¿** | â­ ç®€å• | â­â­ ä¸­ç­‰ | â­â­â­ è¾ƒéš¾ |
| **ç”Ÿæ€ç³»ç»Ÿ** | ğŸŒŸ æˆç†Ÿä¸°å¯Œ | ğŸŒ± å¿«é€Ÿæˆé•¿ | ğŸŒŸ ä¼ä¸šçº§ |
| **æœ€ä½³åœºæ™¯** | åŸå‹å¼€å‘ | å•æœºå¤§æ•°æ® | åˆ†å¸ƒå¼å¤§æ•°æ® |

---

## ä¸ºä»€ä¹ˆé€‰æ‹© Polarsï¼Ÿ

### Polars çš„ä¼˜åŠ¿ ğŸš€

1. **æå¿«çš„é€Ÿåº¦** - æ¯” Pandas å¿« 5-10 å€
2. **å†…å­˜é«˜æ•ˆ** - ä½¿ç”¨ Apache Arrow æ ¼å¼
3. **å¹¶è¡Œå¤„ç†** - è‡ªåŠ¨åˆ©ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
4. **å»¶è¿Ÿæ‰§è¡Œ** - åƒ PySpark ä¸€æ ·ä¼˜åŒ–æŸ¥è¯¢è®¡åˆ’
5. **è¡¨è¾¾å¼ API** - æ›´ç°ä»£ã€æ›´ä¼˜é›…çš„è¯­æ³•
6. **æ—  GIL é™åˆ¶** - Rust å®ç°ï¼ŒçœŸæ­£çš„å¹¶è¡Œ

### ä½•æ—¶ä½¿ç”¨å“ªä¸ªï¼Ÿ

```
æ•°æ®é‡ < 5GB    â†’ Pandas (æœ€ç®€å•)
æ•°æ®é‡ 5-50GB   â†’ Polars (æœ€å¿«) â­ æ¨è
æ•°æ®é‡ 50-100GB â†’ Polars + Streaming
æ•°æ®é‡ > 100GB  â†’ PySpark (åˆ†å¸ƒå¼)
```

---

## å¸¸ç”¨æ“ä½œä¸‰æ–¹å¯¹æ¯”

### 1. å®‰è£…

**Pandas:**
```bash
pip install pandas
```

**Polars:**
```bash
pip install polars
```

**PySpark:**
```bash
pip install pyspark
```

---

### 2. åˆ›å»º DataFrame

**Pandas:**
```python
import pandas as pd

df = pd.DataFrame({
    'id': [1, 2, 3],
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'salary': [70000, 80000, 90000]
})
```

**Polars:**
```python
import polars as pl

df = pl.DataFrame({
    'id': [1, 2, 3],
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'salary': [70000, 80000, 90000]
})
```

**PySpark:**
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

df = spark.createDataFrame([
    (1, 'Alice', 25, 70000),
    (2, 'Bob', 30, 80000),
    (3, 'Charlie', 35, 90000)
], ['id', 'name', 'age', 'salary'])
```

---

### 3. æŸ¥çœ‹æ•°æ®

**Pandas:**
```python
df.head()           # å‰ 5 è¡Œ
df.head(10)         # å‰ 10 è¡Œ
df.info()           # æ•°æ®ä¿¡æ¯
df.describe()       # ç»Ÿè®¡æè¿°
df.shape            # (è¡Œæ•°, åˆ—æ•°)
```

**Polars:**
```python
df.head()           # å‰ 5 è¡Œ
df.head(10)         # å‰ 10 è¡Œ
df.describe()       # ç»Ÿè®¡æè¿°
df.shape            # (è¡Œæ•°, åˆ—æ•°)
df.glimpse()        # ç±»ä¼¼ info() çš„æ¦‚è§ˆ â­
```

**PySpark:**
```python
df.show()           # é»˜è®¤ 20 è¡Œ
df.show(10)         # å‰ 10 è¡Œ
df.printSchema()    # æ•°æ®ç»“æ„
df.describe().show() # ç»Ÿè®¡æè¿°
df.count()          # è¡Œæ•°ï¼ˆéœ€è¦è®¡ç®—ï¼‰
```

---

### 4. è¿‡æ»¤æ•°æ® â­â­â­

**Pandas:**
```python
# å•æ¡ä»¶
df_filtered = df[df['age'] > 25]

# å¤šæ¡ä»¶
df_filtered = df[(df['age'] > 25) & (df['salary'] > 75000)]

# ä½¿ç”¨ query
df_filtered = df.query('age > 25 and salary > 75000')
```

**Polars:**
```python
import polars as pl

# æ–¹æ³• 1ï¼šfilter() - æ¨è â­
df_filtered = df.filter(pl.col('age') > 25)

# æ–¹æ³• 2ï¼šå¤šæ¡ä»¶ - è¶…çº§ä¼˜é›…ï¼
df_filtered = df.filter(
    (pl.col('age') > 25) & (pl.col('salary') > 75000)
)

# æ–¹æ³• 3ï¼šé“¾å¼è°ƒç”¨
df_filtered = (
    df
    .filter(pl.col('age') > 25)
    .filter(pl.col('salary') > 75000)
)
```

**PySpark:**
```python
from pyspark.sql.functions import col

# æ–¹æ³• 1ï¼šfilter() + col()
df_filtered = df.filter(col('age') > 25)

# æ–¹æ³• 2ï¼šå¤šæ¡ä»¶
df_filtered = df.filter(
    (col('age') > 25) & (col('salary') > 75000)
)
```

**è¯­æ³•å¯¹æ¯”ï¼š**
```python
# Pandas:  df[df['age'] > 25]
# Polars:  df.filter(pl.col('age') > 25)  â† æ›´æ¸…æ™°
# PySpark: df.filter(col('age') > 25)
```

---

### 5. é€‰æ‹©åˆ—

**Pandas:**
```python
# å•åˆ—ï¼ˆè¿”å› Seriesï¼‰
df['name']

# å¤šåˆ—ï¼ˆè¿”å› DataFrameï¼‰
df[['name', 'age']]
```

**Polars:**
```python
# å•åˆ—ï¼ˆè¿”å› Seriesï¼‰
df['name']
df.select('name')

# å¤šåˆ—ï¼ˆè¿”å› DataFrameï¼‰
df.select(['name', 'age'])
df.select(pl.col('name'), pl.col('age'))

# æ­£åˆ™é€‰æ‹©ï¼ˆé«˜çº§ï¼‰â­
df.select(pl.col('^.*e$'))  # é€‰æ‹©ä»¥ 'e' ç»“å°¾çš„åˆ—
```

**PySpark:**
```python
# å•åˆ—
df.select('name')

# å¤šåˆ—
df.select('name', 'age')
df.select(col('name'), col('age'))
```

---

### 6. æ·»åŠ æ–°åˆ—

**Pandas:**
```python
# æ–¹æ³• 1ï¼šç›´æ¥èµ‹å€¼
df['age_plus_10'] = df['age'] + 10

# æ–¹æ³• 2ï¼šassign()
df = df.assign(age_plus_10=df['age'] + 10)

# æ–¹æ³• 3ï¼šapply()
df['age_category'] = df['age'].apply(lambda x: 'Senior' if x > 30 else 'Junior')
```

**Polars:**
```python
# æ–¹æ³• 1ï¼šwith_columns() - æ¨è â­
df = df.with_columns(
    (pl.col('age') + 10).alias('age_plus_10')
)

# æ–¹æ³• 2ï¼šæ·»åŠ å¤šåˆ—
df = df.with_columns([
    (pl.col('age') + 10).alias('age_plus_10'),
    (pl.col('salary') * 1.1).alias('salary_increased')
])

# æ–¹æ³• 3ï¼šæ¡ä»¶åˆ—
df = df.with_columns(
    pl.when(pl.col('age') > 30)
      .then(pl.lit('Senior'))
      .otherwise(pl.lit('Junior'))
      .alias('age_category')
)
```

**PySpark:**
```python
# ä½¿ç”¨ withColumn()
df = df.withColumn('age_plus_10', col('age') + 10)

# æ¡ä»¶åˆ—
from pyspark.sql.functions import when, lit

df = df.withColumn('age_category',
    when(col('age') > 30, lit('Senior'))
    .otherwise(lit('Junior'))
)
```

---

### 7. åˆ†ç»„èšåˆ â­â­â­

**Pandas:**
```python
# å•ä¸ªèšåˆ
df.groupby('name')['salary'].mean()

# å¤šä¸ªèšåˆ
df.groupby('name').agg({
    'age': 'mean',
    'salary': ['sum', 'count', 'mean']
})

# ä½¿ç”¨ agg å‡½æ•°
df.groupby('name').agg(
    avg_age=('age', 'mean'),
    total_salary=('salary', 'sum')
)
```

**Polars:**
```python
# æ–¹æ³• 1ï¼šç®€æ´ä¼˜é›… â­
df.groupby('name').agg([
    pl.col('age').mean().alias('avg_age'),
    pl.col('salary').sum().alias('total_salary'),
    pl.col('salary').count().alias('count')
])

# æ–¹æ³• 2ï¼šå¤šä¸ªèšåˆ
df.groupby('name').agg([
    pl.mean('age'),
    pl.sum('salary'),
    pl.count()
])

# æ–¹æ³• 3ï¼šæ¡ä»¶èšåˆ
df.groupby('name').agg([
    pl.col('salary').filter(pl.col('age') > 30).mean().alias('avg_salary_senior')
])
```

**PySpark:**
```python
from pyspark.sql.functions import mean, sum, count

df.groupBy('name').agg(
    mean('age').alias('avg_age'),
    sum('salary').alias('total_salary'),
    count('*').alias('count')
)
```

**é€Ÿåº¦å¯¹æ¯”ï¼š**
```
å¤§æ•°æ®é›†åˆ†ç»„èšåˆé€Ÿåº¦ï¼š
Polars > PySpark (å•æœº) > Pandas
  1x      0.8x               0.1x
```

---

### 8. æ’åº

**Pandas:**
```python
# å‡åº
df.sort_values('age')

# é™åº
df.sort_values('age', ascending=False)

# å¤šåˆ—
df.sort_values(['name', 'age'], ascending=[True, False])
```

**Polars:**
```python
# å‡åº
df.sort('age')

# é™åº
df.sort('age', descending=True)

# å¤šåˆ—
df.sort(['name', 'age'], descending=[False, True])

# ä½¿ç”¨è¡¨è¾¾å¼ â­
df.sort(pl.col('age').cast(pl.Int32))
```

**PySpark:**
```python
from pyspark.sql.functions import desc, asc

# å‡åº
df.orderBy('age')

# é™åº
df.orderBy(desc('age'))

# å¤šåˆ—
df.orderBy('name', desc('age'))
```

---

### 9. JOIN æ“ä½œ

**Pandas:**
```python
# Inner join
pd.merge(df1, df2, on='id')

# Left join
pd.merge(df1, df2, on='id', how='left')

# å¤šä¸ªé”®
pd.merge(df1, df2, on=['id', 'name'])
```

**Polars:**
```python
# Inner join
df1.join(df2, on='id')

# Left join
df1.join(df2, on='id', how='left')

# å¤šä¸ªé”®
df1.join(df2, on=['id', 'name'])

# é«˜çº§ï¼šä½¿ç”¨è¡¨è¾¾å¼
df1.join(
    df2,
    left_on='user_id',
    right_on='id',
    how='left'
)
```

**PySpark:**
```python
# Inner join
df1.join(df2, 'id')

# Left join
df1.join(df2, 'id', 'left')

# å¤šä¸ªé”®
df1.join(df2, ['id', 'name'])
```

---

### 10. å¤„ç†ç¼ºå¤±å€¼

**Pandas:**
```python
# åˆ é™¤ç¼ºå¤±å€¼
df.dropna()
df.dropna(subset=['age'])

# å¡«å……ç¼ºå¤±å€¼
df.fillna(0)
df.fillna({'age': 0, 'name': 'Unknown'})
```

**Polars:**
```python
# åˆ é™¤ç¼ºå¤±å€¼
df.drop_nulls()
df.drop_nulls(subset=['age'])

# å¡«å……ç¼ºå¤±å€¼
df.fill_null(0)
df.fill_null({'age': 0, 'name': 'Unknown'})

# é«˜çº§å¡«å…… â­
df.with_columns([
    pl.col('age').fill_null(pl.col('age').mean())
])
```

**PySpark:**
```python
# åˆ é™¤ç¼ºå¤±å€¼
df.dropna()
df.dropna(subset=['age'])

# å¡«å……ç¼ºå¤±å€¼
df.fillna(0)
df.fillna({'age': 0, 'name': 'Unknown'})
```

---

### 11. å­—ç¬¦ä¸²æ“ä½œ

**Pandas:**
```python
# è½¬å¤§å†™
df['name'] = df['name'].str.upper()

# åŒ…å«åˆ¤æ–­
df[df['name'].str.contains('Alice')]

# åˆ†å‰²
df['name'].str.split(' ')
```

**Polars:**
```python
# è½¬å¤§å†™
df = df.with_columns(
    pl.col('name').str.to_uppercase().alias('name_upper')
)

# åŒ…å«åˆ¤æ–­
df.filter(pl.col('name').str.contains('Alice'))

# åˆ†å‰²
df = df.with_columns(
    pl.col('name').str.split(' ').alias('name_parts')
)
```

**PySpark:**
```python
from pyspark.sql.functions import upper, split

# è½¬å¤§å†™
df = df.withColumn('name_upper', upper(col('name')))

# åŒ…å«åˆ¤æ–­
df.filter(col('name').contains('Alice'))

# åˆ†å‰²
df = df.withColumn('name_parts', split(col('name'), ' '))
```

---

### 12. è¯»å†™æ–‡ä»¶

**Pandas:**
```python
# CSV
df = pd.read_csv('data.csv')
df.to_csv('output.csv', index=False)

# Parquet
df = pd.read_parquet('data.parquet')
df.to_parquet('output.parquet')
```

**Polars:**
```python
# CSV
df = pl.read_csv('data.csv')
df.write_csv('output.csv')

# Parquet - æ¨è â­
df = pl.read_parquet('data.parquet')
df.write_parquet('output.parquet')

# Lazy reading (å¤§æ–‡ä»¶) â­â­
df = pl.scan_parquet('data.parquet').collect()
```

**PySpark:**
```python
# CSV
df = spark.read.csv('data.csv', header=True, inferSchema=True)
df.write.mode('overwrite').csv('output.csv', header=True)

# Parquet
df = spark.read.parquet('data.parquet')
df.write.mode('overwrite').parquet('output.parquet')
```

---

## Polars ç‹¬ç‰¹åŠŸèƒ½ ğŸŒŸ

### 1. Lazy Evaluation (å»¶è¿Ÿæ‰§è¡Œ)

```python
# Lazy API - æ„å»ºæŸ¥è¯¢è®¡åˆ’ï¼Œä¼˜åŒ–åæ‰§è¡Œ
lazy_df = pl.scan_csv('big_file.csv')

result = (
    lazy_df
    .filter(pl.col('age') > 25)
    .select(['name', 'salary'])
    .groupby('name')
    .agg(pl.sum('salary'))
    .collect()  # è¿™é‡Œæ‰çœŸæ­£æ‰§è¡Œ
)

# æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
print(lazy_df.explain())
```

### 2. è¡¨è¾¾å¼å¼ºå¤§çš„ç»„åˆ

```python
# å¤æ‚çš„åˆ—æ“ä½œ
df = df.with_columns([
    # æ¡ä»¶é€»è¾‘
    pl.when(pl.col('age') > 30)
      .then(pl.col('salary') * 1.2)
      .otherwise(pl.col('salary'))
      .alias('adjusted_salary'),

    # èšåˆè¡¨è¾¾å¼
    (pl.col('salary') - pl.col('salary').mean())
      .alias('salary_deviation'),

    # çª—å£å‡½æ•°
    pl.col('salary').rank().over('department').alias('rank_in_dept')
])
```

### 3. Streaming Mode (æµå¼å¤„ç†)

```python
# å¤„ç†è¶…å¤§æ–‡ä»¶ï¼ˆä¸éœ€è¦å…¨éƒ¨åŠ è½½åˆ°å†…å­˜ï¼‰
result = (
    pl.scan_csv('huge_file.csv')
    .filter(pl.col('value') > 100)
    .groupby('category')
    .agg(pl.sum('value'))
    .collect(streaming=True)  # æµå¼å¤„ç†
)
```

### 4. å¹¶è¡Œå¤„ç†

```python
# Polars è‡ªåŠ¨å¹¶è¡Œå¤„ç†ï¼Œæ— éœ€é…ç½®
# è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ

# å¯ä»¥æŸ¥çœ‹å¹¶è¡Œåº¦
pl.threadpool_size()  # æŸ¥çœ‹çº¿ç¨‹æ± å¤§å°
```

---

## æ€§èƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯• ğŸï¸

### åœºæ™¯ï¼š10GB CSV æ–‡ä»¶ï¼Œåˆ†ç»„èšåˆ

```python
# æ•°æ®é›†ï¼š1äº¿è¡Œï¼Œ10 åˆ—

# Pandas
# æ—¶é—´ï¼š~180 ç§’
# å†…å­˜ï¼š~15GB

# Polars (Eager)
# æ—¶é—´ï¼š~25 ç§’  â† å¿« 7 å€ï¼
# å†…å­˜ï¼š~8GB

# Polars (Lazy + Streaming)
# æ—¶é—´ï¼š~30 ç§’
# å†…å­˜ï¼š~2GB  â† å†…å­˜æ•ˆç‡æé«˜ï¼

# PySpark (å•æœº)
# æ—¶é—´ï¼š~40 ç§’
# å†…å­˜ï¼š~10GB
```

---

## è¿ç§»æŒ‡å—

### Pandas â†’ Polars

```python
# Pandas
import pandas as pd
df = pd.read_csv('data.csv')
result = df[df['age'] > 25].groupby('name')['salary'].mean()

# Polarsï¼ˆå‡ ä¹ä¸€æ ·çš„é€»è¾‘ï¼‰
import polars as pl
df = pl.read_csv('data.csv')
result = df.filter(pl.col('age') > 25).groupby('name').agg(pl.mean('salary'))
```

**ä¸»è¦åŒºåˆ«ï¼š**
1. `df['col']` â†’ `pl.col('col')`
2. `df[df['col'] > 0]` â†’ `df.filter(pl.col('col') > 0)`
3. `.apply()` â†’ `.map()` æˆ–è¡¨è¾¾å¼

---

## ä½•æ—¶ä½¿ç”¨å“ªä¸ªï¼Ÿå†³ç­–æ ‘ ğŸŒ²

```
å¼€å§‹
 â”‚
 â”œâ”€ æ•°æ®é‡ < 5GBï¼Ÿ
 â”‚   â”œâ”€ Yes â†’ éœ€è¦å¿«é€ŸåŸå‹ï¼Ÿ
 â”‚   â”‚         â”œâ”€ Yes â†’ Pandas (æœ€ç®€å•)
 â”‚   â”‚         â””â”€ No  â†’ Polars (æ›´å¿«)
 â”‚   â”‚
 â”‚   â””â”€ No â†’ æ•°æ®é‡ < 50GBï¼Ÿ
 â”‚             â”œâ”€ Yes â†’ Polars â­ (å•æœºæœ€ä¼˜)
 â”‚             â””â”€ No  â†’ æ•°æ®é‡ > 100GBï¼Ÿ
 â”‚                       â”œâ”€ Yes â†’ PySpark (åˆ†å¸ƒå¼)
 â”‚                       â””â”€ No  â†’ Polars Streaming
```

---

## æ€»ç»“è¡¨æ ¼

| åœºæ™¯ | æ¨èå·¥å…· | åŸå›  |
|------|---------|------|
| **å­¦ä¹ æ•°æ®åˆ†æ** | Pandas | ç®€å•ï¼Œèµ„æ–™å¤š |
| **ç”Ÿäº§ç¯å¢ƒï¼ˆå•æœºï¼‰** | Polars | å¿«ï¼Œå†…å­˜å°‘ |
| **è¶…å¤§æ•°æ®é›†ï¼ˆé›†ç¾¤ï¼‰** | PySpark | åˆ†å¸ƒå¼ |
| **å¿«é€ŸåŸå‹** | Pandas | ç†Ÿæ‚‰ï¼Œå¿«é€Ÿ |
| **æ€§èƒ½ä¼˜åŒ–** | Polars | é€Ÿåº¦å¿« 5-10x |
| **å®æ—¶æ•°æ®æµ** | PySpark Streaming | ä¼ä¸šçº§ |

---

## å­¦ä¹ å»ºè®® ğŸ“š

### æ¨èå­¦ä¹ é¡ºåºï¼š

1. **Pandas** (1-2 å‘¨) - æ‰“åŸºç¡€
2. **Polars** (1 å‘¨) - è¿ç§»å¾ˆå®¹æ˜“
3. **PySpark** (2-3 å‘¨) - ç†è§£åˆ†å¸ƒå¼

### å®è·µé¡¹ç›®å»ºè®®ï¼š

```python
# åˆçº§ï¼šç”¨ Pandas
# - åˆ†æ CSV æ–‡ä»¶ï¼ˆ< 1GBï¼‰
# - æ•°æ®æ¸…æ´—å’Œå¯è§†åŒ–

# ä¸­çº§ï¼šç”¨ Polars
# - å¤„ç†å¤§æ–‡ä»¶ï¼ˆ5-20GBï¼‰
# - æ€§èƒ½ä¼˜åŒ–æŒ‘æˆ˜

# é«˜çº§ï¼šç”¨ PySpark
# - åˆ†å¸ƒå¼æ•°æ®å¤„ç†
# - å®æ—¶æ•°æ®æµå¤„ç†
```

---

## é€ŸæŸ¥è¡¨

| æ“ä½œ | Pandas | Polars | PySpark |
|------|--------|--------|---------|
| **è¿‡æ»¤** | `df[df['col'] > 0]` | `df.filter(pl.col('col') > 0)` | `df.filter(col('col') > 0)` |
| **é€‰æ‹©** | `df[['a', 'b']]` | `df.select(['a', 'b'])` | `df.select('a', 'b')` |
| **æ·»åŠ åˆ—** | `df['new'] = df['old'] + 1` | `df.with_columns((pl.col('old') + 1).alias('new'))` | `df.withColumn('new', col('old') + 1)` |
| **åˆ†ç»„** | `df.groupby('col').mean()` | `df.groupby('col').agg(pl.mean('*'))` | `df.groupBy('col').mean()` |
| **æ’åº** | `df.sort_values('col')` | `df.sort('col')` | `df.orderBy('col')` |
| **å»é‡** | `df.drop_duplicates()` | `df.unique()` | `df.dropDuplicates()` |

---

## æ¨èèµ„æº

- **Pandas æ–‡æ¡£**: https://pandas.pydata.org/
- **Polars æ–‡æ¡£**: https://pola-rs.github.io/polars/
- **PySpark æ–‡æ¡£**: https://spark.apache.org/docs/latest/api/python/

**Polars æ˜¯æœªæ¥è¶‹åŠ¿ï¼å¼ºçƒˆæ¨èå­¦ä¹ ï¼** ğŸš€
