{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrame é«˜çº§æ“ä½œ\n",
    "\n",
    "## æ•°æ®æ¸…æ´—ä¸è½¬æ¢å®æˆ˜æ•™ç¨‹\n",
    "\n",
    "æœ¬æ•™ç¨‹æ¶µç›–ï¼š\n",
    "- ğŸ§¹ æ•°æ®æ¸…æ´—ï¼ˆç¼ºå¤±å€¼ã€é‡å¤å€¼ã€å¼‚å¸¸å€¼ï¼‰\n",
    "- ğŸ”„ æ•°æ®è½¬æ¢ï¼ˆç±»å‹è½¬æ¢ã€åˆ—æ“ä½œï¼‰\n",
    "- ğŸ¯ é«˜çº§é€‰æ‹©ä¸è¿‡æ»¤\n",
    "- ğŸ”— å¤æ‚æ•°æ®ç±»å‹å¤„ç†\n",
    "- ğŸ› ï¸ å®ç”¨æŠ€å·§ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒè®¾ç½®\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# åˆ›å»º Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame é«˜çº§æ“ä½œ\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark {spark.version} å·²å¯åŠ¨\")\n",
    "print(f\"âœ… Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®\n",
    "\n",
    "åˆ›å»ºä¸€ä¸ªåŒ…å«å„ç§æ•°æ®è´¨é‡é—®é¢˜çš„æ•°æ®é›†ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåŒ…å«æ•°æ®è´¨é‡é—®é¢˜çš„ç”¨æˆ·æ•°æ®\n",
    "messy_data = [\n",
    "    (1, \"Alice\", \"alice@email.com\", 28, \"Female\", 75000.0, \"2023-01-15\", \"New York\"),\n",
    "    (2, \"Bob\", \"bob@email.com\", 35, \"Male\", 85000.0, \"2023-02-20\", \"Los Angeles\"),\n",
    "    (3, \"Charlie\", None, None, \"Male\", 65000.0, \"2023-03-10\", \"Chicago\"),  # ç¼ºå¤±é‚®ç®±å’Œå¹´é¾„\n",
    "    (4, \"David\", \"david@email.com\", 42, \"Male\", None, \"2023-04-05\", \"Houston\"),  # ç¼ºå¤±è–ªèµ„\n",
    "    (5, None, \"eve@email.com\", 29, \"Female\", 70000.0, None, \"Phoenix\"),  # ç¼ºå¤±å§“åå’Œæ—¥æœŸ\n",
    "    (6, \"Frank\", \"frank@invalid\", -5, \"Male\", 72000.0, \"2023-06-15\", \"Philadelphia\"),  # å¼‚å¸¸å¹´é¾„\n",
    "    (7, \"Grace\", \"grace@email.com\", 31, \"Female\", 78000.0, \"2023-07-20\", \"San Antonio\"),\n",
    "    (2, \"Bob\", \"bob@email.com\", 35, \"Male\", 85000.0, \"2023-02-20\", \"Los Angeles\"),  # é‡å¤è¡Œ\n",
    "    (8, \"Henry\", \"henry@email.com\", 150, \"Male\", 90000.0, \"2023-08-25\", \"San Diego\"),  # å¼‚å¸¸å¹´é¾„\n",
    "    (9, \"Ivy\", \"ivy@email.com\", 26, \"Female\", -10000.0, \"2023-09-30\", \"Dallas\"),  # å¼‚å¸¸è–ªèµ„\n",
    "    (10, \"Jack\", \"JACK@EMAIL.COM\", 33, \"male\", 82000.0, \"2023-10-15\", \"san jose\"),  # å¤§å°å†™ä¸ä¸€è‡´\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"email\", \"age\", \"gender\", \"salary\", \"join_date\", \"city\"]\n",
    "\n",
    "df = spark.createDataFrame(messy_data, columns)\n",
    "\n",
    "print(\"åŸå§‹æ•°æ®ï¼ˆåŒ…å«è´¨é‡é—®é¢˜ï¼‰ï¼š\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"\\næ•°æ®æ¦‚è§ˆï¼š\")\n",
    "df.printSchema()\n",
    "print(f\"æ€»è¡Œæ•°: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®è´¨é‡æ£€æŸ¥\n",
    "\n",
    "åœ¨æ¸…æ´—ä¹‹å‰ï¼Œå…ˆäº†è§£æ•°æ®çš„é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æ£€æŸ¥æ¯åˆ—çš„ç¼ºå¤±å€¼æ•°é‡\n",
    "print(\"=\" * 60)\n",
    "print(\"ç¼ºå¤±å€¼ç»Ÿè®¡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pyspark.sql.functions import col, sum as _sum, count, when, isnan\n",
    "\n",
    "# ç»Ÿè®¡æ¯åˆ—çš„ NULL å€¼\n",
    "null_counts = df.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "print(\"æ¯åˆ—çš„ NULL å€¼æ•°é‡ï¼š\")\n",
    "null_counts.show()\n",
    "\n",
    "# 2. æ£€æŸ¥é‡å¤è¡Œ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"é‡å¤æ•°æ®æ£€æŸ¥\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_rows = df.count()\n",
    "distinct_rows = df.distinct().count()\n",
    "duplicate_rows = total_rows - distinct_rows\n",
    "\n",
    "print(f\"æ€»è¡Œæ•°: {total_rows}\")\n",
    "print(f\"å”¯ä¸€è¡Œæ•°: {distinct_rows}\")\n",
    "print(f\"é‡å¤è¡Œæ•°: {duplicate_rows}\")\n",
    "\n",
    "# 3. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"æ•°å€¼åˆ—ç»Ÿè®¡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df.select(\"age\", \"salary\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•°æ®æ¸…æ´—\n",
    "\n",
    "### 3.1 å¤„ç†é‡å¤æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³• 1ï¼šåˆ é™¤å®Œå…¨é‡å¤çš„è¡Œ\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "\n",
    "print(\"åˆ é™¤é‡å¤è¡Œåï¼š\")\n",
    "print(f\"åŸå§‹è¡Œæ•°: {df.count()} â†’ æ¸…æ´—å: {df_no_duplicates.count()}\")\n",
    "\n",
    "# æ–¹æ³• 2ï¼šåŸºäºç‰¹å®šåˆ—åˆ é™¤é‡å¤ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼‰\n",
    "df_no_dup_id = df.dropDuplicates([\"id\"])\n",
    "\n",
    "print(f\"\\nåŸºäº ID åˆ é™¤é‡å¤å: {df_no_dup_id.count()} è¡Œ\")\n",
    "\n",
    "# ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®ç»§ç»­\n",
    "df_clean = df_no_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å¤„ç†ç¼ºå¤±å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"å¤„ç†ç¼ºå¤±å€¼\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ–¹æ³• 1ï¼šåˆ é™¤åŒ…å«ä»»ä½• NULL çš„è¡Œ\n",
    "df_dropna_any = df_clean.dropna(how='any')\n",
    "print(f\"åˆ é™¤ä»»ä½•åŒ…å« NULL çš„è¡Œ: {df_dropna_any.count()} è¡Œ\")\n",
    "\n",
    "# æ–¹æ³• 2ï¼šåˆ é™¤æ‰€æœ‰åˆ—éƒ½æ˜¯ NULL çš„è¡Œ\n",
    "df_dropna_all = df_clean.dropna(how='all')\n",
    "print(f\"åˆ é™¤å…¨éƒ¨ä¸º NULL çš„è¡Œ: {df_dropna_all.count()} è¡Œ\")\n",
    "\n",
    "# æ–¹æ³• 3ï¼šåˆ é™¤ç‰¹å®šåˆ—ä¸º NULL çš„è¡Œ\n",
    "df_dropna_subset = df_clean.dropna(subset=[\"name\", \"email\"])\n",
    "print(f\"åˆ é™¤ name æˆ– email ä¸º NULL çš„è¡Œ: {df_dropna_subset.count()} è¡Œ\")\n",
    "\n",
    "# æ–¹æ³• 4ï¼šå¡«å……ç¼ºå¤±å€¼\n",
    "print(\"\\nå¡«å……ç¼ºå¤±å€¼ç¤ºä¾‹ï¼š\")\n",
    "\n",
    "# ç”¨å›ºå®šå€¼å¡«å……\n",
    "df_filled = df_clean.fillna({\n",
    "    \"name\": \"Unknown\",\n",
    "    \"email\": \"no-email@example.com\",\n",
    "    \"age\": 0,\n",
    "    \"salary\": 0.0,\n",
    "    \"join_date\": \"1900-01-01\"\n",
    "})\n",
    "\n",
    "print(\"ç”¨å›ºå®šå€¼å¡«å……åï¼š\")\n",
    "df_filled.show(truncate=False)\n",
    "\n",
    "# æ–¹æ³• 5ï¼šç”¨å¹³å‡å€¼/ä¸­ä½æ•°å¡«å……æ•°å€¼åˆ—\n",
    "from pyspark.sql.functions import mean, median, approx_count_distinct\n",
    "\n",
    "# è®¡ç®—å¹³å‡å¹´é¾„å’Œè–ªèµ„\n",
    "avg_age = df_clean.select(mean(\"age\")).first()[0]\n",
    "avg_salary = df_clean.select(mean(\"salary\")).first()[0]\n",
    "\n",
    "df_filled_avg = df_clean.fillna({\n",
    "    \"age\": avg_age,\n",
    "    \"salary\": avg_salary\n",
    "})\n",
    "\n",
    "print(f\"\\nç”¨å¹³å‡å€¼å¡«å……: å¹´é¾„={avg_age:.1f}, è–ªèµ„={avg_salary:.2f}\")\n",
    "df_filled_avg.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 å¤„ç†å¼‚å¸¸å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"å¤„ç†å¼‚å¸¸å€¼\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. è¯†åˆ«å¼‚å¸¸å€¼ï¼ˆå¹´é¾„åº”è¯¥åœ¨ 18-100 ä¹‹é—´ï¼‰\n",
    "df_age_check = df_filled_avg.withColumn(\n",
    "    \"age_valid\",\n",
    "    when((col(\"age\") >= 18) & (col(\"age\") <= 100), True).otherwise(False)\n",
    ")\n",
    "\n",
    "print(\"å¹´é¾„å¼‚å¸¸å€¼æ£€æŸ¥ï¼š\")\n",
    "df_age_check.select(\"name\", \"age\", \"age_valid\").show()\n",
    "\n",
    "# 2. è¿‡æ»¤å¼‚å¸¸å€¼\n",
    "df_no_outliers = df_filled_avg.filter(\n",
    "    (col(\"age\") >= 18) & (col(\"age\") <= 100) &\n",
    "    (col(\"salary\") >= 0)\n",
    ")\n",
    "\n",
    "print(f\"\\nè¿‡æ»¤å¼‚å¸¸å€¼å: {df_no_outliers.count()} è¡Œ\")\n",
    "df_no_outliers.show(truncate=False)\n",
    "\n",
    "# 3. ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•è¯†åˆ«å¼‚å¸¸å€¼ï¼ˆIQR æ–¹æ³•ï¼‰\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "\n",
    "# è®¡ç®—å››åˆ†ä½æ•°\n",
    "quantiles = df_filled_avg.approxQuantile(\"salary\", [0.25, 0.75], 0.05)\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nè–ªèµ„å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆIQR æ–¹æ³•ï¼‰ï¼š\")\n",
    "print(f\"Q1 (25%): {Q1:.2f}\")\n",
    "print(f\"Q3 (75%): {Q3:.2f}\")\n",
    "print(f\"IQR: {IQR:.2f}\")\n",
    "print(f\"æ­£å¸¸èŒƒå›´: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "df_iqr_filtered = df_filled_avg.filter(\n",
    "    (col(\"salary\") >= lower_bound) & (col(\"salary\") <= upper_bound)\n",
    ")\n",
    "\n",
    "print(f\"\\nä½¿ç”¨ IQR è¿‡æ»¤å: {df_iqr_filtered.count()} è¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å››éƒ¨åˆ†ï¼šæ•°æ®è½¬æ¢\n",
    "\n",
    "### 4.1 åˆ—æ“ä½œï¼ˆæ·»åŠ ã€åˆ é™¤ã€é‡å‘½åï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"åˆ—æ“ä½œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. æ·»åŠ æ–°åˆ—\n",
    "df_with_cols = df_no_outliers \\\n",
    "    .withColumn(\"salary_category\", \n",
    "                when(col(\"salary\") < 70000, \"Junior\")\n",
    "                .when(col(\"salary\") < 80000, \"Mid\")\n",
    "                .otherwise(\"Senior\")) \\\n",
    "    .withColumn(\"age_group\",\n",
    "                when(col(\"age\") < 30, \"20s\")\n",
    "                .when(col(\"age\") < 40, \"30s\")\n",
    "                .otherwise(\"40+\")) \\\n",
    "    .withColumn(\"full_name_upper\", upper(col(\"name\")))\n",
    "\n",
    "print(\"æ·»åŠ æ–°åˆ—åï¼š\")\n",
    "df_with_cols.select(\"name\", \"age\", \"salary\", \"age_group\", \"salary_category\").show()\n",
    "\n",
    "# 2. åˆ é™¤åˆ—\n",
    "df_dropped = df_with_cols.drop(\"full_name_upper\")\n",
    "print(f\"\\nåˆ é™¤åˆ—å: {len(df_dropped.columns)} åˆ—\")\n",
    "\n",
    "# 3. é‡å‘½ååˆ—\n",
    "df_renamed = df_with_cols \\\n",
    "    .withColumnRenamed(\"name\", \"employee_name\") \\\n",
    "    .withColumnRenamed(\"salary\", \"annual_salary\")\n",
    "\n",
    "print(\"\\né‡å‘½ååˆ—åï¼š\")\n",
    "print(df_renamed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å­—ç¬¦ä¸²å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"å­—ç¬¦ä¸²å¤„ç†\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. å¤§å°å†™è½¬æ¢\n",
    "df_string = df_no_outliers \\\n",
    "    .withColumn(\"name_upper\", upper(col(\"name\"))) \\\n",
    "    .withColumn(\"name_lower\", lower(col(\"name\"))) \\\n",
    "    .withColumn(\"email_lower\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"city_proper\", initcap(col(\"city\")))  # é¦–å­—æ¯å¤§å†™\n",
    "\n",
    "print(\"å­—ç¬¦ä¸²è½¬æ¢ï¼š\")\n",
    "df_string.select(\"name\", \"name_upper\", \"name_lower\", \"city\", \"city_proper\").show(truncate=False)\n",
    "\n",
    "# 2. å»é™¤ç©ºæ ¼\n",
    "df_trimmed = df_no_outliers \\\n",
    "    .withColumn(\"name_trimmed\", trim(col(\"name\"))) \\\n",
    "    .withColumn(\"email_trimmed\", trim(col(\"email\")))\n",
    "\n",
    "# 3. å­—ç¬¦ä¸²åˆ†å‰²\n",
    "df_split = df_no_outliers \\\n",
    "    .withColumn(\"email_parts\", split(col(\"email\"), \"@\")) \\\n",
    "    .withColumn(\"email_user\", split(col(\"email\"), \"@\")[0]) \\\n",
    "    .withColumn(\"email_domain\", split(col(\"email\"), \"@\")[1])\n",
    "\n",
    "print(\"\\nå­—ç¬¦ä¸²åˆ†å‰²ï¼ˆé‚®ç®±ï¼‰ï¼š\")\n",
    "df_split.select(\"email\", \"email_user\", \"email_domain\").show(truncate=False)\n",
    "\n",
    "# 4. å­—ç¬¦ä¸²æ‹¼æ¥\n",
    "df_concat = df_no_outliers \\\n",
    "    .withColumn(\"full_info\", \n",
    "                concat_ws(\" - \", col(\"name\"), col(\"city\"), col(\"gender\")))\n",
    "\n",
    "print(\"\\nå­—ç¬¦ä¸²æ‹¼æ¥ï¼š\")\n",
    "df_concat.select(\"full_info\").show(truncate=False)\n",
    "\n",
    "# 5. æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢\n",
    "df_regex = df_no_outliers \\\n",
    "    .withColumn(\"email_masked\",\n",
    "                regexp_replace(col(\"email\"), \"@.*\", \"@***\"))\n",
    "\n",
    "print(\"\\né‚®ç®±è„±æ•ï¼š\")\n",
    "df_regex.select(\"email\", \"email_masked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ç±»å‹è½¬æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ç±»å‹è½¬æ¢\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æŸ¥çœ‹å½“å‰æ•°æ®ç±»å‹\n",
    "print(\"åŸå§‹æ•°æ®ç±»å‹ï¼š\")\n",
    "df_no_outliers.printSchema()\n",
    "\n",
    "# 1. è½¬æ¢æ—¥æœŸç±»å‹\n",
    "df_typed = df_no_outliers \\\n",
    "    .withColumn(\"join_date\", to_date(col(\"join_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# 2. è½¬æ¢æ•°å€¼ç±»å‹\n",
    "df_typed = df_typed \\\n",
    "    .withColumn(\"age\", col(\"age\").cast(IntegerType())) \\\n",
    "    .withColumn(\"salary\", col(\"salary\").cast(DoubleType()))\n",
    "\n",
    "print(\"\\nè½¬æ¢åçš„æ•°æ®ç±»å‹ï¼š\")\n",
    "df_typed.printSchema()\n",
    "\n",
    "# 3. ä»æ—¥æœŸä¸­æå–ä¿¡æ¯\n",
    "df_date = df_typed \\\n",
    "    .withColumn(\"year\", year(col(\"join_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"join_date\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"join_date\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"join_date\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"join_date\")))\n",
    "\n",
    "print(\"\\næ—¥æœŸä¿¡æ¯æå–ï¼š\")\n",
    "df_date.select(\"name\", \"join_date\", \"year\", \"month\", \"day\", \"quarter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 æ¡ä»¶è½¬æ¢å’Œ CASE WHEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"æ¡ä»¶è½¬æ¢ (CASE WHEN)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. ç®€å•æ¡ä»¶\n",
    "df_conditions = df_typed \\\n",
    "    .withColumn(\"is_senior\",\n",
    "                when(col(\"age\") >= 35, \"Yes\").otherwise(\"No\")) \\\n",
    "    .withColumn(\"salary_level\",\n",
    "                when(col(\"salary\") >= 85000, \"High\")\n",
    "                .when(col(\"salary\") >= 75000, \"Medium-High\")\n",
    "                .when(col(\"salary\") >= 70000, \"Medium\")\n",
    "                .otherwise(\"Low\"))\n",
    "\n",
    "print(\"æ¡ä»¶åˆ†ç±»ï¼š\")\n",
    "df_conditions.select(\"name\", \"age\", \"is_senior\", \"salary\", \"salary_level\").show()\n",
    "\n",
    "# 2. å¤æ‚æ¡ä»¶ç»„åˆ\n",
    "df_complex = df_typed \\\n",
    "    .withColumn(\"talent_category\",\n",
    "                when((col(\"age\") < 30) & (col(\"salary\") >= 75000), \"Rising Star\")\n",
    "                .when((col(\"age\") >= 35) & (col(\"salary\") >= 80000), \"Senior Expert\")\n",
    "                .when(col(\"salary\") >= 85000, \"High Performer\")\n",
    "                .otherwise(\"Regular\"))\n",
    "\n",
    "print(\"\\näººæ‰åˆ†ç±»ï¼š\")\n",
    "df_complex.select(\"name\", \"age\", \"salary\", \"talent_category\").show()\n",
    "\n",
    "# 3. ä½¿ç”¨ SQL è¡¨è¾¾å¼\n",
    "df_typed.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        age,\n",
    "        salary,\n",
    "        CASE\n",
    "            WHEN salary >= 85000 THEN 'ğŸ’° é«˜è–ª'\n",
    "            WHEN salary >= 75000 THEN 'ğŸ’µ ä¸­é«˜è–ª'\n",
    "            ELSE 'ğŸ’´ æ™®é€š'\n",
    "        END as salary_emoji\n",
    "    FROM employees\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSQL æ¡ä»¶è¡¨è¾¾å¼ï¼š\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äº”éƒ¨åˆ†ï¼šé«˜çº§é€‰æ‹©ä¸è¿‡æ»¤\n",
    "\n",
    "### 5.1 å¤æ‚è¿‡æ»¤æ¡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"é«˜çº§è¿‡æ»¤\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. å¤šæ¡ä»¶è¿‡æ»¤\n",
    "filtered_1 = df_typed.filter(\n",
    "    (col(\"age\") >= 30) & \n",
    "    (col(\"salary\") >= 75000) & \n",
    "    (col(\"gender\") == \"Female\")\n",
    ")\n",
    "\n",
    "print(\"30å²ä»¥ä¸Šã€è–ªèµ„>=75000 çš„å¥³æ€§ï¼š\")\n",
    "filtered_1.show()\n",
    "\n",
    "# 2. IN æ“ä½œ\n",
    "filtered_2 = df_typed.filter(col(\"city\").isin([\"New York\", \"Los Angeles\", \"Chicago\"]))\n",
    "\n",
    "print(\"\\nç‰¹å®šåŸå¸‚çš„å‘˜å·¥ï¼š\")\n",
    "filtered_2.show()\n",
    "\n",
    "# 3. LIKE æ¨¡ç³ŠåŒ¹é…\n",
    "filtered_3 = df_typed.filter(col(\"email\").like(\"%@email.com\"))\n",
    "\n",
    "print(\"\\né‚®ç®±åŒ…å« @email.com çš„å‘˜å·¥ï¼š\")\n",
    "filtered_3.show()\n",
    "\n",
    "# 4. æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…\n",
    "filtered_4 = df_typed.filter(col(\"name\").rlike(\"^[A-C].*\"))  # åå­—ä»¥ Aã€Bã€C å¼€å¤´\n",
    "\n",
    "print(\"\\nåå­—ä»¥ A/B/C å¼€å¤´çš„å‘˜å·¥ï¼š\")\n",
    "filtered_4.show()\n",
    "\n",
    "# 5. BETWEEN\n",
    "filtered_5 = df_typed.filter(col(\"salary\").between(70000, 80000))\n",
    "\n",
    "print(\"\\nè–ªèµ„åœ¨ 70000-80000 ä¹‹é—´ï¼š\")\n",
    "filtered_5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 åŠ¨æ€åˆ—é€‰æ‹©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"åŠ¨æ€åˆ—é€‰æ‹©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. é€‰æ‹©ç‰¹å®šåˆ—\n",
    "selected_1 = df_typed.select(\"name\", \"age\", \"salary\")\n",
    "print(\"åŸºæœ¬é€‰æ‹©ï¼š\")\n",
    "selected_1.show(3)\n",
    "\n",
    "# 2. ä½¿ç”¨è¡¨è¾¾å¼é€‰æ‹©\n",
    "selected_2 = df_typed.select(\n",
    "    col(\"name\"),\n",
    "    (col(\"salary\") / 12).alias(\"monthly_salary\"),\n",
    "    (col(\"salary\") * 0.2).alias(\"annual_bonus\")\n",
    ")\n",
    "\n",
    "print(\"\\nè®¡ç®—åˆ—ï¼š\")\n",
    "selected_2.show()\n",
    "\n",
    "# 3. æ’é™¤ç‰¹å®šåˆ—\n",
    "cols_to_keep = [c for c in df_typed.columns if c not in [\"id\", \"email\"]]\n",
    "selected_3 = df_typed.select(cols_to_keep)\n",
    "\n",
    "print(\"\\næ’é™¤ id å’Œ emailï¼š\")\n",
    "print(selected_3.columns)\n",
    "\n",
    "# 4. é€‰æ‹©ç‰¹å®šç±»å‹çš„åˆ—\n",
    "numeric_cols = [field.name for field in df_typed.schema.fields \n",
    "                if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))]\n",
    "\n",
    "print(f\"\\næ•°å€¼åˆ—: {numeric_cols}\")\n",
    "df_typed.select(numeric_cols).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å…­éƒ¨åˆ†ï¼šå®æˆ˜æ¡ˆä¾‹\n",
    "\n",
    "ç»¼åˆè¿ç”¨æ•°æ®æ¸…æ´—å’Œè½¬æ¢æŠ€å·§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"å®æˆ˜æ¡ˆä¾‹ï¼šå®Œæ•´çš„æ•°æ®æ¸…æ´—æµç¨‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ­¥éª¤ 1: åˆ é™¤é‡å¤æ•°æ®\n",
    "df_step1 = df.dropDuplicates()\n",
    "print(f\"æ­¥éª¤ 1 - åˆ é™¤é‡å¤: {df.count()} â†’ {df_step1.count()} è¡Œ\")\n",
    "\n",
    "# æ­¥éª¤ 2: æ ‡å‡†åŒ–å­—ç¬¦ä¸²ï¼ˆå°å†™ã€å»ç©ºæ ¼ï¼‰\n",
    "df_step2 = df_step1 \\\n",
    "    .withColumn(\"email\", lower(trim(col(\"email\")))) \\\n",
    "    .withColumn(\"gender\", initcap(trim(col(\"gender\")))) \\\n",
    "    .withColumn(\"city\", initcap(trim(col(\"city\"))))\n",
    "\n",
    "print(\"æ­¥éª¤ 2 - å­—ç¬¦ä¸²æ ‡å‡†åŒ–å®Œæˆ\")\n",
    "\n",
    "# æ­¥éª¤ 3: å¤„ç†å¼‚å¸¸å€¼\n",
    "df_step3 = df_step2.filter(\n",
    "    (col(\"age\").isNull() | ((col(\"age\") >= 18) & (col(\"age\") <= 100))) &\n",
    "    (col(\"salary\").isNull() | (col(\"salary\") >= 0))\n",
    ")\n",
    "\n",
    "print(f\"æ­¥éª¤ 3 - è¿‡æ»¤å¼‚å¸¸å€¼: {df_step2.count()} â†’ {df_step3.count()} è¡Œ\")\n",
    "\n",
    "# æ­¥éª¤ 4: å¡«å……ç¼ºå¤±å€¼\n",
    "avg_age = df_step3.select(mean(\"age\")).first()[0]\n",
    "avg_salary = df_step3.select(mean(\"salary\")).first()[0]\n",
    "\n",
    "df_step4 = df_step3.fillna({\n",
    "    \"name\": \"Unknown\",\n",
    "    \"email\": \"unknown@example.com\",\n",
    "    \"age\": avg_age,\n",
    "    \"salary\": avg_salary,\n",
    "    \"join_date\": \"2023-01-01\"\n",
    "})\n",
    "\n",
    "print(f\"æ­¥éª¤ 4 - å¡«å……ç¼ºå¤±å€¼å®Œæˆ (avg_age={avg_age:.1f}, avg_salary={avg_salary:.2f})\")\n",
    "\n",
    "# æ­¥éª¤ 5: ç±»å‹è½¬æ¢\n",
    "df_step5 = df_step4 \\\n",
    "    .withColumn(\"join_date\", to_date(col(\"join_date\"))) \\\n",
    "    .withColumn(\"age\", col(\"age\").cast(IntegerType())) \\\n",
    "    .withColumn(\"salary\", col(\"salary\").cast(DoubleType()))\n",
    "\n",
    "print(\"æ­¥éª¤ 5 - ç±»å‹è½¬æ¢å®Œæˆ\")\n",
    "\n",
    "# æ­¥éª¤ 6: æ·»åŠ æ´¾ç”Ÿåˆ—\n",
    "df_final = df_step5 \\\n",
    "    .withColumn(\"salary_category\",\n",
    "                when(col(\"salary\") >= 85000, \"High\")\n",
    "                .when(col(\"salary\") >= 75000, \"Medium\")\n",
    "                .otherwise(\"Low\")) \\\n",
    "    .withColumn(\"age_group\",\n",
    "                when(col(\"age\") < 30, \"20s\")\n",
    "                .when(col(\"age\") < 40, \"30s\")\n",
    "                .otherwise(\"40+\")) \\\n",
    "    .withColumn(\"join_year\", year(col(\"join_date\"))) \\\n",
    "    .withColumn(\"email_domain\", split(col(\"email\"), \"@\")[1]) \\\n",
    "    .withColumn(\"monthly_salary\", round(col(\"salary\") / 12, 2))\n",
    "\n",
    "print(\"æ­¥éª¤ 6 - æ·»åŠ æ´¾ç”Ÿåˆ—å®Œæˆ\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"æœ€ç»ˆæ¸…æ´—åçš„æ•°æ®\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_final.show(truncate=False)\n",
    "\n",
    "print(\"\\næœ€ç»ˆæ•°æ®ç»Ÿè®¡ï¼š\")\n",
    "print(f\"æ€»è¡Œæ•°: {df_final.count()}\")\n",
    "print(f\"æ€»åˆ—æ•°: {len(df_final.columns)}\")\n",
    "print(f\"åˆ—å: {df_final.columns}\")\n",
    "\n",
    "# ä¿å­˜æ¸…æ´—åçš„æ•°æ®\n",
    "# df_final.write.mode(\"overwrite\").parquet(\"/tmp/cleaned_employees\")\n",
    "# print(\"\\nâœ… æ•°æ®å·²ä¿å­˜åˆ° /tmp/cleaned_employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€å·§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"æ€§èƒ½ä¼˜åŒ–æŠ€å·§\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. ä½¿ç”¨ cache() ç¼“å­˜é¢‘ç¹ä½¿ç”¨çš„ DataFrame\n",
    "df_cached = df_final.cache()\n",
    "print(\"âœ… DataFrame å·²ç¼“å­˜\")\n",
    "\n",
    "# 2. æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’\n",
    "print(\"\\næŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ï¼š\")\n",
    "df_final.filter(col(\"salary\") > 75000).explain(mode=\"simple\")\n",
    "\n",
    "# 3. ä½¿ç”¨ persist() æ§åˆ¶å­˜å‚¨çº§åˆ«\n",
    "from pyspark import StorageLevel\n",
    "df_persisted = df_final.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(\"\\nâœ… DataFrame å·²æŒä¹…åŒ–åˆ°å†…å­˜å’Œç£ç›˜\")\n",
    "\n",
    "# 4. åˆ†åŒºä¼˜åŒ–\n",
    "print(f\"\\nå½“å‰åˆ†åŒºæ•°: {df_final.rdd.getNumPartitions()}\")\n",
    "\n",
    "# é‡æ–°åˆ†åŒºï¼ˆé€‚ç”¨äºå¤§æ•°æ®é›†ï¼‰\n",
    "df_repartitioned = df_final.repartition(4)\n",
    "print(f\"é‡æ–°åˆ†åŒºå: {df_repartitioned.rdd.getNumPartitions()} ä¸ªåˆ†åŒº\")\n",
    "\n",
    "# 5. æ€§èƒ½æç¤º\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"æ€§èƒ½ä¼˜åŒ–å»ºè®®\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. å°½æ—©è¿‡æ»¤æ•°æ® (filter/where)\n",
    "2. é€‰æ‹©éœ€è¦çš„åˆ— (select)\n",
    "3. ç¼“å­˜é¢‘ç¹ä½¿ç”¨çš„ DataFrame (cache/persist)\n",
    "4. åˆç†è®¾ç½®åˆ†åŒºæ•° (repartition/coalesce)\n",
    "5. é¿å…ä½¿ç”¨ collect() å’Œ show() åœ¨å¤§æ•°æ®é›†ä¸Š\n",
    "6. ä½¿ç”¨ broadcast join å¤„ç†å°è¡¨\n",
    "7. ä½¿ç”¨ Parquet æ ¼å¼å­˜å‚¨æ•°æ®\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹ é¢˜\n",
    "\n",
    "### ç»ƒä¹  1ï¼šæ•°æ®æ¸…æ´—\n",
    "åˆ›å»ºä¸€ä¸ªåŒ…å«è´¨é‡é—®é¢˜çš„è®¢å•æ•°æ®ï¼Œå¹¶æ¸…æ´—å®ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ æ•°æ®\n",
    "order_data = [\n",
    "    (1, \"ORD001\", \"2024-01-15\", 100.50, \"Alice\"),\n",
    "    (2, \"ORD002\", \"2024-01-16\", None, \"Bob\"),  # ç¼ºå¤±é‡‘é¢\n",
    "    (3, \"ORD003\", None, 200.00, \"Charlie\"),  # ç¼ºå¤±æ—¥æœŸ\n",
    "    (1, \"ORD001\", \"2024-01-15\", 100.50, \"Alice\"),  # é‡å¤\n",
    "    (4, \"ORD004\", \"2024-01-17\", -50.00, \"David\"),  # å¼‚å¸¸é‡‘é¢\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(order_data, [\"id\", \"order_id\", \"order_date\", \"amount\", \"customer\"])\n",
    "\n",
    "print(\"åŸå§‹è®¢å•æ•°æ®ï¼š\")\n",
    "orders_df.show()\n",
    "\n",
    "# TODO: å®Œæˆä»¥ä¸‹ä»»åŠ¡\n",
    "# 1. åˆ é™¤é‡å¤è®¢å•\n",
    "# 2. è¿‡æ»¤æ‰é‡‘é¢ä¸ºè´Ÿçš„è®¢å•\n",
    "# 3. ç”¨å¹³å‡é‡‘é¢å¡«å……ç¼ºå¤±çš„ amount\n",
    "# 4. ç”¨ '2024-01-01' å¡«å……ç¼ºå¤±çš„æ—¥æœŸ\n",
    "# 5. æ·»åŠ ä¸€ä¸ª amount_category åˆ—ï¼ˆHigh: >150, Medium: 100-150, Low: <100ï¼‰\n",
    "\n",
    "# ä½ çš„ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹  2ï¼šå­—ç¬¦ä¸²å¤„ç†\n",
    "å¤„ç†å®¢æˆ·ä¿¡æ¯ä¸­çš„å­—ç¬¦ä¸²æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ æ•°æ®\n",
    "customer_data = [\n",
    "    (1, \"  ALICE SMITH  \", \"alice.smith@GMAIL.com\", \"NEW YORK\"),\n",
    "    (2, \"bob jones\", \"BOB@yahoo.com\", \"los angeles\"),\n",
    "    (3, \"Charlie Brown\", \"charlie@Email.COM\", \"Chicago\"),\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, [\"id\", \"name\", \"email\", \"city\"])\n",
    "\n",
    "print(\"åŸå§‹å®¢æˆ·æ•°æ®ï¼š\")\n",
    "customers_df.show(truncate=False)\n",
    "\n",
    "# TODO: å®Œæˆä»¥ä¸‹ä»»åŠ¡\n",
    "# 1. å»é™¤ name çš„é¦–å°¾ç©ºæ ¼\n",
    "# 2. å°† name è½¬æ¢ä¸ºæ ‡é¢˜æ ¼å¼ï¼ˆæ¯ä¸ªå•è¯é¦–å­—æ¯å¤§å†™ï¼‰\n",
    "# 3. å°† email å…¨éƒ¨è½¬æ¢ä¸ºå°å†™\n",
    "# 4. ä» email ä¸­æå–ç”¨æˆ·åå’ŒåŸŸå\n",
    "# 5. å°† city è½¬æ¢ä¸ºæ ‡é¢˜æ ¼å¼\n",
    "\n",
    "# ä½ çš„ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "### æœ¬æ•™ç¨‹å­¦åˆ°çš„æ ¸å¿ƒæŠ€èƒ½ï¼š\n",
    "\n",
    "#### æ•°æ®æ¸…æ´—\n",
    "- âœ… å¤„ç†ç¼ºå¤±å€¼ï¼š`dropna()`, `fillna()`\n",
    "- âœ… åˆ é™¤é‡å¤ï¼š`dropDuplicates()`\n",
    "- âœ… å¼‚å¸¸å€¼æ£€æµ‹ï¼šè¿‡æ»¤ã€IQR æ–¹æ³•\n",
    "\n",
    "#### æ•°æ®è½¬æ¢\n",
    "- âœ… åˆ—æ“ä½œï¼š`withColumn()`, `drop()`, `withColumnRenamed()`\n",
    "- âœ… å­—ç¬¦ä¸²å¤„ç†ï¼š`upper()`, `lower()`, `trim()`, `split()`\n",
    "- âœ… ç±»å‹è½¬æ¢ï¼š`cast()`, `to_date()`\n",
    "- âœ… æ¡ä»¶é€»è¾‘ï¼š`when().otherwise()`\n",
    "\n",
    "#### é«˜çº§æŠ€å·§\n",
    "- âœ… å¤æ‚è¿‡æ»¤ï¼š`filter()`, `isin()`, `like()`, `between()`\n",
    "- âœ… åŠ¨æ€åˆ—é€‰æ‹©\n",
    "- âœ… æ€§èƒ½ä¼˜åŒ–ï¼š`cache()`, `repartition()`\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¦ä¹ \n",
    "1. çª—å£å‡½æ•°ï¼ˆæ’åã€ç§»åŠ¨å¹³å‡ï¼‰\n",
    "2. å¤æ‚æ•°æ®ç±»å‹ï¼ˆæ•°ç»„ã€ç»“æ„ä½“ã€Mapï¼‰\n",
    "3. JOIN ä¼˜åŒ–æŠ€å·§\n",
    "4. å®æˆ˜é¡¹ç›®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…³é—­ Spark Session\n",
    "# spark.stop()\n",
    "# print(\"âœ… Spark Session å·²å…³é—­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•\n",
    "pwd\n",
    "# åº”è¯¥æ˜¾ç¤ºï¼š/Users/zhengzhang\n",
    "\n",
    "# 2. ï¼ˆå¯é€‰ï¼‰æ¸…ç† Notebook è¾“å‡º\n",
    "jupyter nbconvert --clear-output --inplace \"Documents/2026/data engineer/learn pyspark/pep8.ipynb\"\n",
    "\n",
    "# 3. æŸ¥çœ‹ä¿®æ”¹äº†ä»€ä¹ˆ\n",
    "git status\n",
    "\n",
    "# 4. æŸ¥çœ‹å…·ä½“æ”¹åŠ¨ï¼ˆå¯é€‰ï¼‰\n",
    "git diff \"Documents/2026/data engineer/learn pyspark/pep8.ipynb\"\n",
    "\n",
    "# 5. æ·»åŠ ä¿®æ”¹\n",
    "git add \"Documents/2026/data engineer/learn pyspark/pep8.ipynb\"\n",
    "\n",
    "# 6. æäº¤\n",
    "git commit -m \"Update: æ›´æ–° PEP 8 ç¬”è®°å†…å®¹\"\n",
    "\n",
    "# 7. æ¨é€åˆ° GitHub\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3613438092.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/37/nk_nn77n1z30jhrdt6m_mr380000gn/T/ipykernel_53664/3613438092.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git add \"Documents/2026/data engineer/learn pyspark/ySpark_DataFrame_Advanced.ipynb\"\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "PySpark_DataFrame_Advanced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
